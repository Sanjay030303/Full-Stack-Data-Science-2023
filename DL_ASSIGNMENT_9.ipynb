{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPhLSNcQYrnt2FMaGAqfEHv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sanjay030303/Full-Stack-Data-Science-2023/blob/main/DL_ASSIGNMENT_9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEbbcABmu5Cw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **What are the main tasks that autoencoders are used for?**\n",
        "   - Autoencoders are used for dimensionality reduction, data denoising, anomaly detection, and as a pretraining step for other models, such as in unsupervised feature learning.\n",
        "\n",
        "2. **Suppose you want to train a classifier, and you have plenty of unlabeled training data but only a few thousand labeled instances. How can autoencoders help? How would you proceed?**\n",
        "   - Autoencoders can help by learning useful features from the unlabeled data in an unsupervised manner. You can proceed by:\n",
        "     1. Training an autoencoder on the unlabeled data to learn a compressed representation.\n",
        "     2. Using the encoder part of the trained autoencoder to transform both the labeled and unlabeled data into this lower-dimensional space.\n",
        "     3. Training a classifier on the labeled data using these learned features.\n",
        "     4. Optionally, fine-tuning the classifier with semi-supervised learning techniques, leveraging the transformed unlabeled data.\n",
        "\n",
        "3. **If an autoencoder perfectly reconstructs the inputs, is it necessarily a good autoencoder? How can you evaluate the performance of an autoencoder?**\n",
        "   - A perfect reconstruction does not necessarily mean it is a good autoencoder. It might have simply memorized the inputs rather than learning meaningful features. Performance can be evaluated by:\n",
        "     - Reconstruction loss: How well the autoencoder reconstructs unseen data.\n",
        "     - Latent space quality: Visualizing and analyzing the learned representations.\n",
        "     - Downstream task performance: Using the encoder's output in tasks like classification or clustering and measuring accuracy.\n",
        "\n",
        "4. **What are undercomplete and overcomplete autoencoders? What is the main risk of an excessively undercomplete autoencoder? What about the main risk of an overcomplete autoencoder?**\n",
        "   - **Undercomplete autoencoders:** Have a bottleneck layer smaller than the input, forcing the model to learn efficient representations.\n",
        "     - **Main risk:** It may not capture all important features, leading to underfitting.\n",
        "   - **Overcomplete autoencoders:** Have a bottleneck layer larger than the input, allowing more capacity for learning.\n",
        "     - **Main risk:** It may memorize the input data rather than learning generalizable features, leading to overfitting.\n",
        "\n",
        "5. **How do you tie weights in a stacked autoencoder? What is the point of doing so?**\n",
        "   - Tied weights mean that the weights of the encoder are constrained to be the transpose of the weights of the decoder. This reduces the number of parameters, ensuring that the encoder and decoder learn inverse mappings and can help improve the generalization ability of the model.\n",
        "\n",
        "6. **What is a generative model? Can you name a type of generative autoencoder?**\n",
        "   - A generative model generates new data instances that resemble the training data. A type of generative autoencoder is the **Variational Autoencoder (VAE)**, which learns to generate new samples from the learned data distribution.\n",
        "\n",
        "7. **What is a GAN? Can you name a few tasks where GANs can shine?**\n",
        "   - A **Generative Adversarial Network (GAN)** consists of two networks, a generator and a discriminator, that train together in a competitive manner. GANs can shine in tasks like:\n",
        "     - Image generation and enhancement.\n",
        "     - Style transfer.\n",
        "     - Text-to-image synthesis.\n",
        "     - Data augmentation.\n",
        "     - Video prediction and generation.\n",
        "\n",
        "8. **What are the main difficulties when training GANs?**\n",
        "   - **Instability:** The training process can be highly unstable, with the generator and discriminator often getting stuck in a loop of adversarial improvements.\n",
        "   - **Mode collapse:** The generator might produce a limited variety of outputs, failing to capture the diversity of the data distribution.\n",
        "   - **Balancing:** Ensuring that the generator and discriminator improve at a similar pace is challenging; if one becomes too strong, the other cannot learn effectively.\n",
        "   - **Hyperparameter tuning:** GANs require careful tuning of hyperparameters, such as learning rates and architecture configurations."
      ],
      "metadata": {
        "id": "nmUV5WIcu50E"
      }
    }
  ]
}