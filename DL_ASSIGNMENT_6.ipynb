{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO2MQxPjYQUpOGbct30g4r9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sanjay030303/Full-Stack-Data-Science-2023/blob/main/DL_ASSIGNMENT_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEbbcABmu5Cw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **What are the advantages of a CNN over a fully connected DNN for image classification?**\n",
        "   - CNNs use convolutional layers to capture spatial hierarchies in images, reducing parameters and computation. This makes them more efficient and better at recognizing patterns in images than fully connected DNNs.\n",
        "\n",
        "2. **Consider a CNN composed of three convolutional layers, each with 3 × 3 kernels, a stride of 2, and \"same\" padding. The lowest layer outputs 100 feature maps, the middle one outputs 200, and the top one outputs 400. The input images are RGB images of 200 × 300 pixels. What is the total number of parameters in the CNN? If we are using 32-bit floats, at least how much RAM will this network require when making a prediction for a single instance? What about when training on a mini-batch of 50 images?**\n",
        "   - Total number of parameters: \\[ \\text{Layer 1: } (3 \\times 3 \\times 3 \\times 100) + 100 = 2800 \\]\n",
        "\\[ \\text{Layer 2: } (3 \\times 3 \\times 100 \\times 200) + 200 = 180200 \\]\n",
        "\\[ \\text{Layer 3: } (3 \\times 3 \\times 200 \\times 400) + 400 = 720400 \\]\n",
        "\\[ \\text{Total: } 2800 + 180200 + 720400 = 903400 \\]\n",
        "\n",
        "   - RAM for a single instance:\n",
        "\\[ \\text{Feature maps: } (200 \\times 300 \\times 3) + (100 \\times 100 \\times 100) + (50 \\times 50 \\times 200) + (25 \\times 25 \\times 400) = 180000 + 1000000 + 500000 + 250000 = 1930000 \\text{ elements} \\]\n",
        "\\[ \\text{Total bytes: } 1930000 \\times 4 = 7720000 \\text{ bytes} \\approx 7.72 \\text{ MB} \\]\n",
        "\n",
        "   - RAM for mini-batch of 50 images:\n",
        "\\[ 7.72 \\text{ MB} \\times 50 = 386 \\text{ MB} \\]\n",
        "\n",
        "3. **If your GPU runs out of memory while training a CNN, what are five things you could try to solve the problem?**\n",
        "   - Reduce the batch size.\n",
        "   - Use mixed precision training.\n",
        "   - Reduce the size or number of layers.\n",
        "   - Use gradient checkpointing.\n",
        "   - Offload some computations to the CPU.\n",
        "\n",
        "4. **Why would you want to add a max pooling layer rather than a convolutional layer with the same stride?**\n",
        "   - Max pooling layers reduce the spatial dimensions, decrease the number of parameters, and help make the network invariant to small translations in the input.\n",
        "\n",
        "5. **When would you want to add a local response normalization layer?**\n",
        "   - Local response normalization is useful when you want to encourage competition for large activations in a region, which can help in cases like image recognition to enhance feature maps with high activations.\n",
        "\n",
        "6. **Can you name the main innovations in AlexNet, compared to LeNet-5? What about the main innovations in GoogLeNet, ResNet, SENet, and Xception?**\n",
        "   - **AlexNet:** Introduced ReLU activation, dropout, data augmentation, and used GPUs for training.\n",
        "   - **GoogLeNet:** Introduced the Inception module, which uses multiple filter sizes at each layer.\n",
        "   - **ResNet:** Introduced residual connections to solve the vanishing gradient problem.\n",
        "   - **SENet:** Introduced squeeze-and-excitation blocks to recalibrate channel-wise feature responses.\n",
        "   - **Xception:** Used depthwise separable convolutions to improve efficiency.\n",
        "\n",
        "7. **What is a fully convolutional network? How can you convert a dense layer into a convolutional layer?**\n",
        "   - A fully convolutional network (FCN) uses only convolutional layers, including for the final output. Convert a dense layer into a convolutional layer by using a \\(1 \\times 1\\) convolution with the same number of filters as the number of units in the dense layer.\n",
        "\n",
        "8. **What is the main technical difficulty of semantic segmentation?**\n",
        "   - The main difficulty is achieving precise pixel-level classification, as it requires detailed spatial information and balancing accuracy with computational efficiency.\n",
        "\n",
        "9. **Build your own CNN from scratch and try to achieve the highest possible accuracy on MNIST.**\n",
        "   ```python\n",
        "   import tensorflow as tf\n",
        "   from tensorflow.keras import layers, models\n",
        "\n",
        "   # Load and preprocess MNIST data\n",
        "   (train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
        "   train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
        "   test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
        "\n",
        "   # Build the CNN model\n",
        "   model = models.Sequential([\n",
        "       layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "       layers.MaxPooling2D((2, 2)),\n",
        "       layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "       layers.MaxPooling2D((2, 2)),\n",
        "       layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "       layers.Flatten(),\n",
        "       layers.Dense(64, activation='relu'),\n",
        "       layers.Dense(10, activation='softmax')\n",
        "   ])\n",
        "\n",
        "   # Compile and train the model\n",
        "   model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "   model.fit(train_images, train_labels, epochs=5, batch_size=64, validation_split=0.1)\n",
        "\n",
        "   # Evaluate the model\n",
        "   test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "   print(f'Test accuracy: {test_acc}')\n",
        "   ```\n",
        "\n",
        "10. **Use transfer learning for large image classification, going through these steps:**\n",
        "    - **a. Create a training set containing at least 100 images per class.**\n",
        "      - You can use TensorFlow Datasets like `tfds.load('cats_vs_dogs', split='train[:10%]')` for a small sample.\n",
        "    - **b. Split it into a training set, a validation set, and a test set.**\n",
        "      - Use `tf.data.Dataset` to split the data, e.g., `train_size = 0.8 * dataset_size`.\n",
        "    - **c. Build the input pipeline, including the appropriate preprocessing operations, and optionally add data augmentation.**\n",
        "      - ```python\n",
        "        def preprocess(image, label):\n",
        "            image = tf.image.resize(image, [224, 224])\n",
        "            image = image / 255.0\n",
        "            return image, label\n",
        "\n",
        "        train_dataset = train_dataset.map(preprocess).batch(32).shuffle(buffer_size=1000)\n",
        "        val_dataset = val_dataset.map(preprocess).batch(32)\n",
        "        test_dataset = test_dataset.map(preprocess).batch(32)\n",
        "        ```\n",
        "    - **d. Fine-tune a pretrained model on this dataset.**\n",
        "      - ```python\n",
        "        base_model = tf.keras.applications.MobileNetV2(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
        "        base_model.trainable = False\n",
        "\n",
        "        model = models.Sequential([\n",
        "            base_model,\n",
        "            layers.GlobalAveragePooling2D(),\n",
        "            layers.Dense(1, activation='sigmoid')\n",
        "        ])\n",
        "\n",
        "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "        model.fit(train_dataset, validation_data=val_dataset, epochs=10)\n",
        "\n",
        "        base_model.trainable = True\n",
        "        model.compile(optimizer=tf.keras.optimizers.Adam(1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "        model.fit(train_dataset, validation_data=val_dataset, epochs=10)\n",
        "\n",
        "        test_loss, test_acc = model.evaluate(test_dataset)\n",
        "        print(f'Test accuracy: {test_acc}')\n",
        "        ```"
      ],
      "metadata": {
        "id": "nmUV5WIcu50E"
      }
    }
  ]
}