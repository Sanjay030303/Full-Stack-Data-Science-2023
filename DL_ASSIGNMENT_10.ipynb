{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMhOsTsyRjU7xDXAY4hIzdJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sanjay030303/Full-Stack-Data-Science-2023/blob/main/DL_ASSIGNMENT_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEbbcABmu5Cw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **What does a SavedModel contain? How do you inspect its content?**\n",
        "   - A SavedModel contains:\n",
        "     - The model architecture and computation graph.\n",
        "     - Trained variables (weights and biases).\n",
        "     - Metadata and signatures for serving.\n",
        "   - To inspect its content, you can use TensorFlow's command-line tool `saved_model_cli` or the Python API:\n",
        "     ```bash\n",
        "     saved_model_cli show --dir /path/to/saved_model --all\n",
        "     ```\n",
        "     ```python\n",
        "     import tensorflow as tf\n",
        "     model = tf.saved_model.load(\"/path/to/saved_model\")\n",
        "     print(model.signatures)\n",
        "     ```\n",
        "\n",
        "2. **When should you use TF Serving? What are its main features? What are some tools you can use to deploy it?**\n",
        "   - **When to use:** TF Serving is ideal for deploying machine learning models to production, offering flexible and high-performance serving of models.\n",
        "   - **Main features:**\n",
        "     - Efficient serving of TensorFlow models.\n",
        "     - Supports multiple models and versioning.\n",
        "     - High performance and scalability.\n",
        "     - Provides REST and gRPC APIs.\n",
        "   - **Tools for deployment:**\n",
        "     - Docker: Deploy TF Serving in a containerized environment.\n",
        "     - Kubernetes: For scalable and managed deployment.\n",
        "     - TensorFlow Extended (TFX): End-to-end ML pipeline management.\n",
        "\n",
        "3. **How do you deploy a model across multiple TF Serving instances?**\n",
        "   - Deploying across multiple TF Serving instances involves:\n",
        "     - **Container orchestration:** Use tools like Kubernetes to manage multiple instances.\n",
        "     - **Load balancing:** Set up a load balancer to distribute requests among instances.\n",
        "     - **Shared storage:** Use a shared storage system (e.g., NFS, S3) to ensure all instances can access the model files.\n",
        "     - **Versioning:** Utilize TF Serving’s built-in model versioning to manage updates and rollbacks.\n",
        "\n",
        "4. **When should you use the gRPC API rather than the REST API to query a model served by TF Serving?**\n",
        "   - **Use gRPC API:** When you need low-latency, high-throughput communication, as gRPC is more efficient than REST for binary data and can handle streaming requests.\n",
        "   - **Use REST API:** When you need simplicity, easy integration with web services, or when the client environment does not support gRPC.\n",
        "\n",
        "5. **What are the different ways TFLite reduces a model’s size to make it run on a mobile or embedded device?**\n",
        "   - **Quantization:** Reduces the precision of the model weights and activations (e.g., from 32-bit floats to 8-bit integers).\n",
        "   - **Pruning:** Removes unnecessary or redundant weights in the model.\n",
        "   - **Weight clustering:** Groups similar weights and replaces them with shared values.\n",
        "   - **Model optimization toolkit:** Provides various techniques for optimizing and compressing the model.\n",
        "\n",
        "6. **What is quantization-aware training, and why would you need it?**\n",
        "   - **Quantization-aware training:** Simulates quantization during training to help the model learn to maintain accuracy despite reduced precision.\n",
        "   - **Need:** It helps maintain higher accuracy in the final quantized model compared to post-training quantization, especially for models where precision reduction can significantly affect performance.\n",
        "\n",
        "7. **What are model parallelism and data parallelism? Why is the latter generally recommended?**\n",
        "   - **Model parallelism:** Splits the model across multiple devices, each handling different parts of the model.\n",
        "   - **Data parallelism:** Splits the data across multiple devices, each running a copy of the model.\n",
        "   - **Recommendation:** Data parallelism is generally recommended because it is easier to implement, scales more effectively, and leverages modern hardware architectures better, reducing inter-device communication overhead.\n",
        "\n",
        "8. **When training a model across multiple servers, what distribution strategies can you use? How do you choose which one to use?**\n",
        "   - **Distribution strategies:**\n",
        "     - **Synchronous training:** All nodes wait for each other to complete their work before proceeding (e.g., `tf.distribute.MirroredStrategy`).\n",
        "     - **Asynchronous training:** Nodes work independently, updating the model asynchronously (e.g., `tf.distribute.experimental.ParameterServerStrategy`).\n",
        "   - **Choosing the strategy:**\n",
        "     - Use synchronous training for better model accuracy and stability, especially when batch sizes are small.\n",
        "     - Use asynchronous training for faster training and better utilization of resources, especially when dealing with large-scale data and models."
      ],
      "metadata": {
        "id": "nmUV5WIcu50E"
      }
    }
  ]
}