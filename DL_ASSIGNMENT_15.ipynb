{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOIOmItvGZ45h0YJlezc5E5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sanjay030303/Full-Stack-Data-Science-2023/blob/main/DL_ASSIGNMENT_15.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEbbcABmu5Cw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Deep Learning\n",
        "\n",
        "#### a. Build a DNN with five hidden layers of 100 neurons each, He initialization, and the ELU activation function.\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def create_dnn_model(input_shape, num_classes):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.InputLayer(input_shape=input_shape))\n",
        "    he_init = tf.keras.initializers.HeNormal()\n",
        "\n",
        "    for _ in range(5):\n",
        "        model.add(layers.Dense(100, activation='elu', kernel_initializer=he_init))\n",
        "    \n",
        "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "    return model\n",
        "\n",
        "input_shape = (28 * 28,)\n",
        "num_classes = 5\n",
        "model = create_dnn_model(input_shape, num_classes)\n",
        "```\n",
        "\n",
        "#### b. Train the model on MNIST (digits 0 to 4) using Adam optimization and early stopping.\n",
        "```python\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Filter digits 0 to 4\n",
        "mask_train = y_train < 5\n",
        "mask_test = y_test < 5\n",
        "\n",
        "x_train, y_train = x_train[mask_train], y_train[mask_train]\n",
        "x_test, y_test = x_test[mask_test], y_test[mask_test]\n",
        "\n",
        "x_train = x_train.reshape(-1, 28 * 28)\n",
        "x_test = x_test.reshape(-1, 28 * 28)\n",
        "\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "checkpoint_cb = ModelCheckpoint('dnn_model.h5', save_best_only=True)\n",
        "early_stopping_cb = EarlyStopping(patience=10, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(x_train, y_train, epochs=100, validation_split=0.2, callbacks=[checkpoint_cb, early_stopping_cb])\n",
        "```\n",
        "\n",
        "#### c. Tune the hyperparameters using cross-validation.\n",
        "```python\n",
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "def build_model():\n",
        "    return create_dnn_model(input_shape, num_classes)\n",
        "\n",
        "model_cv = KerasClassifier(build_fn=build_model, epochs=100, batch_size=32, verbose=0)\n",
        "param_grid = {'batch_size': [16, 32, 64], 'epochs': [50, 100]}\n",
        "grid_search = GridSearchCV(estimator=model_cv, param_grid=param_grid, cv=3)\n",
        "grid_result = grid_search.fit(x_train, y_train)\n",
        "\n",
        "best_params = grid_result.best_params_\n",
        "best_score = grid_result.best_score_\n",
        "print(f\"Best Params: {best_params}, Best Score: {best_score}\")\n",
        "```\n",
        "\n",
        "#### d. Add Batch Normalization and compare the learning curves.\n",
        "```python\n",
        "def create_dnn_model_with_bn(input_shape, num_classes):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.InputLayer(input_shape=input_shape))\n",
        "    he_init = tf.keras.initializers.HeNormal()\n",
        "\n",
        "    for _ in range(5):\n",
        "        model.add(layers.Dense(100, kernel_initializer=he_init))\n",
        "        model.add(layers.BatchNormalization())\n",
        "        model.add(layers.Activation('elu'))\n",
        "    \n",
        "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "    return model\n",
        "\n",
        "model_bn = create_dnn_model_with_bn(input_shape, num_classes)\n",
        "model_bn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history_bn = model_bn.fit(x_train, y_train, epochs=100, validation_split=0.2, callbacks=[checkpoint_cb, early_stopping_cb])\n",
        "```\n",
        "\n",
        "#### e. Add dropout to every layer and try again.\n",
        "```python\n",
        "def create_dnn_model_with_dropout(input_shape, num_classes, dropout_rate=0.5):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.InputLayer(input_shape=input_shape))\n",
        "    he_init = tf.keras.initializers.HeNormal()\n",
        "\n",
        "    for _ in range(5):\n",
        "        model.add(layers.Dense(100, kernel_initializer=he_init))\n",
        "        model.add(layers.Activation('elu'))\n",
        "        model.add(layers.Dropout(dropout_rate))\n",
        "    \n",
        "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "    return model\n",
        "\n",
        "model_dropout = create_dnn_model_with_dropout(input_shape, num_classes)\n",
        "model_dropout.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history_dropout = model_dropout.fit(x_train, y_train, epochs=100, validation_split=0.2, callbacks=[checkpoint_cb, early_stopping_cb])\n",
        "```\n",
        "\n",
        "### 2. Transfer Learning\n",
        "\n",
        "#### a. Create a new DNN that reuses all the pretrained hidden layers of the previous model, freezes them, and replaces the softmax output layer with a new one.\n",
        "```python\n",
        "pretrained_model = tf.keras.models.load_model('dnn_model.h5')\n",
        "pretrained_layers = pretrained_model.layers[:-1]\n",
        "\n",
        "new_model = models.Sequential(pretrained_layers)\n",
        "for layer in new_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "new_model.add(layers.Dense(5, activation='softmax'))\n",
        "```\n",
        "\n",
        "#### b. Train this new DNN on digits 5 to 9 using only 100 images per digit.\n",
        "```python\n",
        "mask_train_5to9 = (y_train >= 5) & (y_train <= 9)\n",
        "x_train_5to9, y_train_5to9 = x_train[mask_train_5to9], y_train[mask_train_5to9]\n",
        "\n",
        "new_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history_transfer = new_model.fit(x_train_5to9, y_train_5to9, epochs=100, validation_split=0.2, callbacks=[checkpoint_cb, early_stopping_cb])\n",
        "```\n",
        "\n",
        "#### c. Cache the frozen layers and train the model again.\n",
        "```python\n",
        "new_model_with_cache = models.Sequential(pretrained_layers)\n",
        "for layer in new_model_with_cache.layers:\n",
        "    layer.trainable = False\n",
        "new_model_with_cache.add(layers.Dense(5, activation='softmax'))\n",
        "\n",
        "new_model_with_cache.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "@tf.function\n",
        "def cached_layer_output(x):\n",
        "    return new_model_with_cache.layers[0](x)\n",
        "\n",
        "x_train_cached = cached_layer_output(x_train_5to9)\n",
        "\n",
        "history_transfer_cache = new_model_with_cache.fit(x_train_cached, y_train_5to9, epochs=100, validation_split=0.2, callbacks=[checkpoint_cb, early_stopping_cb])\n",
        "```\n",
        "\n",
        "#### d. Reuse just four hidden layers instead of five.\n",
        "```python\n",
        "new_model_4_layers = models.Sequential(pretrained_layers[:-1])\n",
        "for layer in new_model_4_layers.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "new_model_4_layers.add(layers.Dense(5, activation='softmax'))\n",
        "new_model_4_layers.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history_transfer_4_layers = new_model_4_layers.fit(x_train_5to9, y_train_5to9, epochs=100, validation_split=0.2, callbacks=[checkpoint_cb, early_stopping_cb])\n",
        "```\n",
        "\n",
        "#### e. Unfreeze the top two hidden layers and continue training.\n",
        "```python\n",
        "for layer in new_model.layers[-2:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "new_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history_finetune = new_model.fit(x_train_5to9, y_train_5to9, epochs=100, validation_split=0.2, callbacks=[checkpoint_cb, early_stopping_cb])\n",
        "```\n",
        "\n",
        "### 3. Pretraining on an Auxiliary Task\n",
        "\n",
        "#### a. Build two DNNs without the output layer, then concatenate their outputs and add a new hidden layer and an output layer.\n",
        "```python\n",
        "def create_dnn_without_output(input_shape):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.InputLayer(input_shape=input_shape))\n",
        "    he_init = tf.keras.initializers.HeNormal()\n",
        "\n",
        "    for _ in range(5):\n",
        "        model.add(layers.Dense(100, activation='elu', kernel_initializer=he_init))\n",
        "    return model\n",
        "\n",
        "input_shape = (28 * 28,)\n",
        "dnn_a = create_dnn_without_output(input_shape)\n",
        "dnn_b = create_dnn_without_output(input_shape)\n",
        "\n",
        "combined_input = layers.concatenate([dnn_a.output, dnn_b.output])\n",
        "hidden_layer = layers.Dense(10, activation='elu', kernel_initializer=he_init)(combined_input)\n",
        "output_layer = layers.Dense(1, activation='sigmoid')(hidden_layer)\n",
        "\n",
        "comparison_model = models.Model(inputs=[dnn_a.input, dnn_b.input], outputs=output_layer)\n",
        "```\n",
        "\n",
        "#### b. Create a function to generate training batches of pairs of images.\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "def generate_pairs(x, y, num_pairs):\n",
        "    pairs = []\n",
        "    labels = []\n",
        "    n_classes = len(np.unique(y))\n",
        "    class_idx = [np.where(y == i)[0] for i in range(n_classes)]\n",
        "    \n",
        "    for _ in range(num_pairs // 2):\n",
        "        for digit in range(n_classes):\n",
        "            idx_a, idx_b = np.random.choice(class_idx[digit], 2, replace=False)\n",
        "            pairs += [[x[idx_a], x[idx_b]]]\n",
        "            labels += [0]\n",
        "            \n",
        "            digit_b = (digit + np.random.randint(1, n_classes)) % n_classes\n",
        "           \n",
        "\n",
        "\n",
        "#### c. Train the DNN on the training set of image pairs.\n",
        "```python\n",
        "def generate_pairs_batch(x, y, batch_size):\n",
        "    while True:\n",
        "        pairs = []\n",
        "        labels = []\n",
        "        n_classes = len(np.unique(y))\n",
        "        class_idx = [np.where(y == i)[0] for i in range(n_classes)]\n",
        "\n",
        "        for _ in range(batch_size // 2):\n",
        "            for digit in range(n_classes):\n",
        "                idx_a, idx_b = np.random.choice(class_idx[digit], 2, replace=False)\n",
        "                pairs += [[x[idx_a], x[idx_b]]]\n",
        "                labels += [0]\n",
        "\n",
        "                digit_b = (digit + np.random.randint(1, n_classes)) % n_classes\n",
        "                idx_b = np.random.choice(class_idx[digit_b])\n",
        "                pairs += [[x[idx_a], x[idx_b]]]\n",
        "                labels += [1]\n",
        "\n",
        "        pairs = np.array(pairs)\n",
        "        labels = np.array(labels)\n",
        "        yield [pairs[:, 0], pairs[:, 1]], labels\n",
        "\n",
        "comparison_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "train_pairs_generator = generate_pairs_batch(x_train, y_train, batch_size=32)\n",
        "validation_pairs = ([x_test[:2500], x_test[2500:5000]], y_test[:2500], y_test[2500:5000])\n",
        "\n",
        "history_comparison = comparison_model.fit(train_pairs_generator, steps_per_epoch=len(x_train) // 32, epochs=100, validation_data=validation_pairs)\n",
        "```\n",
        "\n",
        "#### d. Create a new DNN by reusing and freezing the hidden layers of DNN A and add a softmax output layer on top with 10 neurons.\n",
        "```python\n",
        "pretrained_layers_a = dnn_a.layers[:-1]\n",
        "\n",
        "new_model_from_a = models.Sequential(pretrained_layers_a)\n",
        "for layer in new_model_from_a.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "new_model_from_a.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "new_model_from_a.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history_new_model_from_a = new_model_from_a.fit(x_train, y_train, epochs=100, validation_split=0.2, callbacks=[checkpoint_cb, early_stopping_cb])\n",
        "```\n",
        "\n",
        "These steps conclude the exercise on pretraining on an auxiliary task using DNNs for comparing MNIST digit images and then transferring the learned features to a new classification task. This approach leverages the shared lower layers for better performance with limited training data on the new task."
      ],
      "metadata": {
        "id": "nmUV5WIcu50E"
      }
    }
  ]
}