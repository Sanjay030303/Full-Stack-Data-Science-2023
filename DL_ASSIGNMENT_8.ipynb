{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPthdY9VpwC6rGTxneDcBOL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sanjay030303/Full-Stack-Data-Science-2023/blob/main/DL_ASSIGNMENT_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEbbcABmu5Cw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **What are the pros and cons of using a stateful RNN versus a stateless RNN?**\n",
        "   - **Stateful RNN:**\n",
        "     - **Pros:** Maintains state across batches, which is useful for learning long-term dependencies.\n",
        "     - **Cons:** More complex to implement and manage, requires careful handling of state between batches.\n",
        "   - **Stateless RNN:**\n",
        "     - **Pros:** Easier to implement, each batch is independent, making it simpler to reset states.\n",
        "     - **Cons:** Cannot capture long-term dependencies as effectively because state is reset after each batch.\n",
        "\n",
        "2. **Why do people use Encoderâ€“Decoder RNNs rather than plain sequence-to-sequence RNNs for automatic translation?**\n",
        "   - Encoder-Decoder RNNs are used because they can handle variable-length input and output sequences more effectively. The encoder processes the entire input sequence into a fixed-size context vector, which the decoder then uses to generate the output sequence. This separation allows for more flexible handling of different sequence lengths.\n",
        "\n",
        "3. **How can you deal with variable-length input sequences? What about variable-length output sequences?**\n",
        "   - **Variable-length input sequences:**\n",
        "     - Use padding to make all sequences the same length, then mask the padding during processing.\n",
        "     - Use RNNs that can handle sequences without needing padding, such as TensorFlow's `tf.keras.layers.RNN` with masking.\n",
        "   - **Variable-length output sequences:**\n",
        "     - Use techniques like sequence-to-sequence models with attention mechanisms, which can generate outputs of varying lengths based on the input sequence.\n",
        "\n",
        "4. **What is beam search and why would you use it? What tool can you use to implement it?**\n",
        "   - Beam search is a search algorithm that explores multiple possible outputs simultaneously and keeps the most promising candidates. It is used in sequence generation tasks like translation and text generation to improve the quality of the generated sequences by considering multiple possibilities at each step. Tools like TensorFlow and PyTorch have libraries or functions to implement beam search.\n",
        "\n",
        "5. **What is an attention mechanism? How does it help?**\n",
        "   - An attention mechanism allows the model to focus on specific parts of the input sequence when generating each part of the output sequence. This helps by providing context dynamically, improving the handling of long-range dependencies, and allowing the model to align input and output sequences more effectively, leading to better performance in tasks like translation and text summarization.\n",
        "\n",
        "6. **What is the most important layer in the Transformer architecture? What is its purpose?**\n",
        "   - The most important layer in the Transformer architecture is the **multi-head self-attention layer**. Its purpose is to allow the model to weigh the importance of different words in a sequence relative to each other, capturing relationships and dependencies regardless of their distance in the input sequence. This enables the model to understand context more effectively than traditional RNNs.\n",
        "\n",
        "7. **When would you need to use sampled softmax?**\n",
        "   - Sampled softmax is used when dealing with large output vocabularies, such as in language models with thousands or millions of words. Instead of computing the full softmax over all possible outputs, which is computationally expensive, sampled softmax approximates the softmax by considering only a random sample of the possible outputs, making training more efficient while still maintaining good performance."
      ],
      "metadata": {
        "id": "nmUV5WIcu50E"
      }
    }
  ]
}