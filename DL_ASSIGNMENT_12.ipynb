{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMcS/Kp5opVMfP9IGRFpyX3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sanjay030303/Full-Stack-Data-Science-2023/blob/main/DL_ASSIGNMENT_12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEbbcABmu5Cw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **How does `unsqueeze` help us to solve certain broadcasting problems?**\n",
        "   ```python\n",
        "   import torch\n",
        "   a = torch.tensor([1, 2, 3])\n",
        "   b = torch.tensor([[4, 5, 6], [7, 8, 9]])\n",
        "   a_unsqueezed = a.unsqueeze(0)\n",
        "   print(a_unsqueezed)\n",
        "   # Output: tensor([[1, 2, 3]])\n",
        "   ```\n",
        "\n",
        "2. **How can we use indexing to do the same operation as `unsqueeze`?**\n",
        "   ```python\n",
        "   import torch\n",
        "   a = torch.tensor([1, 2, 3])\n",
        "   a_unsqueezed = a[None, :]\n",
        "   print(a_unsqueezed)\n",
        "   # Output: tensor([[1, 2, 3]])\n",
        "   ```\n",
        "\n",
        "3. **How do we show the actual contents of the memory used for a tensor?**\n",
        "   ```python\n",
        "   import torch\n",
        "   a = torch.tensor([1, 2, 3])\n",
        "   print(a.data_ptr())\n",
        "   ```\n",
        "\n",
        "4. **When adding a vector of size 3 to a matrix of size 3Ã—3, are the elements of the vector added to each row or each column of the matrix?**\n",
        "   ```python\n",
        "   import torch\n",
        "   a = torch.tensor([1, 2, 3])\n",
        "   b = torch.tensor([[4, 5, 6], [7, 8, 9], [10, 11, 12]])\n",
        "   result = a + b\n",
        "   print(result)\n",
        "   # Output:\n",
        "   # tensor([[ 5,  7,  9],\n",
        "   #         [ 8, 10, 12],\n",
        "   #         [11, 13, 15]])\n",
        "   ```\n",
        "\n",
        "5. **Do broadcasting and `expand_as` result in increased memory use? Why or why not?**\n",
        "   ```python\n",
        "   # Broadcasting does not result in increased memory use because it creates virtual arrays.\n",
        "   # `expand_as` can result in increased memory use because it physically expands the tensor.\n",
        "   ```\n",
        "\n",
        "6. **Implement `matmul` using Einstein summation.**\n",
        "   ```python\n",
        "   import torch\n",
        "   a = torch.tensor([[1, 2], [3, 4]])\n",
        "   b = torch.tensor([[5, 6], [7, 8]])\n",
        "   result = torch.einsum('ik,kj->ij', a, b)\n",
        "   print(result)\n",
        "   # Output: tensor([[19, 22], [43, 50]])\n",
        "   ```\n",
        "\n",
        "7. **What does a repeated index letter represent on the left-hand side of `einsum`?**\n",
        "   ```python\n",
        "   # A repeated index letter represents a summation over that index.\n",
        "   ```\n",
        "\n",
        "8. **What are the three rules of Einstein summation notation? Why?**\n",
        "   ```python\n",
        "   # 1. Repeated indices imply summation.\n",
        "   # 2. Each index can appear at most twice in any term.\n",
        "   # 3. Free indices (those not summed over) appear in the result.\n",
        "   # These rules simplify tensor operations and reduce notation complexity.\n",
        "   ```\n",
        "\n",
        "9. **What are the forward pass and backward pass of a neural network?**\n",
        "   ```python\n",
        "   # Forward pass: Calculating the output of the network given the input.\n",
        "   # Backward pass: Calculating the gradients for backpropagation to update weights.\n",
        "   ```\n",
        "\n",
        "10. **Why do we need to store some of the activations calculated for intermediate layers in the forward pass?**\n",
        "    ```python\n",
        "    # We need to store intermediate activations to compute gradients during the backward pass.\n",
        "    ```\n",
        "\n",
        "11. **What is the downside of having activations with a standard deviation too far away from 1?**\n",
        "    ```python\n",
        "    # Activations with a standard deviation too far from 1 can cause vanishing or exploding gradients.\n",
        "    ```\n",
        "\n",
        "12. **How can weight initialization help avoid this problem?**\n",
        "    ```python\n",
        "    # Proper weight initialization can help maintain a standard deviation close to 1 for activations, preventing gradient issues.\n",
        "    ```"
      ],
      "metadata": {
        "id": "nmUV5WIcu50E"
      }
    }
  ]
}