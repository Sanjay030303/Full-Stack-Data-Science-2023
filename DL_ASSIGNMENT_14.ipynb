{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMvkelNG5SA44mCCuAOgkfP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sanjay030303/Full-Stack-Data-Science-2023/blob/main/DL_ASSIGNMENT_14.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEbbcABmu5Cw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Is it okay to initialize all the weights to the same value as long as that value is selected randomly using He initialization?**\n",
        "   ```python\n",
        "   # No, it is not okay. Initializing all weights to the same value, even if randomly selected, causes symmetry problems, making neurons learn the same features.\n",
        "   ```\n",
        "\n",
        "2. **Is it okay to initialize the bias terms to 0?**\n",
        "   ```python\n",
        "   # Yes, it is generally okay to initialize bias terms to 0, as it does not break the symmetry problem in neural networks.\n",
        "   ```\n",
        "\n",
        "3. **Name three advantages of the ELU activation function over ReLU.**\n",
        "   ```python\n",
        "   # 1. ELU has negative values, which help the network to converge to zero mean, reducing bias shifts.\n",
        "   # 2. ELU smooths the curve when the activation is less than zero, reducing the vanishing gradient problem.\n",
        "   # 3. ELU can push mean activations closer to zero, helping to speed up learning.\n",
        "   ```\n",
        "\n",
        "4. **In which cases would you want to use each of the following activation functions: ELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?**\n",
        "   ```python\n",
        "   # ELU: For deep neural networks to reduce the vanishing gradient problem and ensure faster convergence.\n",
        "   # Leaky ReLU: For cases where neurons can die with ReLU, providing a small slope when input is negative.\n",
        "   # ReLU: General use in hidden layers of deep neural networks due to simplicity and effectiveness.\n",
        "   # Tanh: When outputs need to be normalized between -1 and 1, often used in recurrent neural networks.\n",
        "   # Logistic: For binary classification tasks, squashing outputs between 0 and 1.\n",
        "   # Softmax: For multi-class classification problems, providing probabilistic outputs.\n",
        "   ```\n",
        "\n",
        "5. **What may happen if you set the `momentum` hyperparameter too close to 1 (e.g., 0.99999) when using a `MomentumOptimizer`?**\n",
        "   ```python\n",
        "   # Setting momentum too close to 1 can cause the optimizer to overshoot the minimum, making the training process unstable and oscillatory.\n",
        "   ```\n",
        "\n",
        "6. **Name three ways you can produce a sparse model.**\n",
        "   ```python\n",
        "   # 1. L1 regularization: Adding an L1 penalty to the loss function encourages sparsity in the model weights.\n",
        "   # 2. Pruning: Removing weights with the smallest magnitudes from the model after training.\n",
        "   # 3. Sparse initialization: Starting with a model where many weights are initially set to zero.\n",
        "   ```\n",
        "\n",
        "7. **Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)?**\n",
        "   ```python\n",
        "   # Dropout does slow down training due to the additional computation required to drop units randomly.\n",
        "   # Dropout does not slow down inference, as it is typically turned off during prediction, using the full network.\n",
        "   ```"
      ],
      "metadata": {
        "id": "nmUV5WIcu50E"
      }
    }
  ]
}