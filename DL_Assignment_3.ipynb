{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPE2g9pey1E03uler37/gLK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sanjay030303/Full-Stack-Data-Science-2023/blob/main/DL_Assignment_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLJolXkONUUX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly! Here are the answers to each question:\n",
        "\n",
        "1. **Is it OK to initialize all the weights to the same value as long as that value is selected randomly using He initialization?**\n",
        "   - No, it's not ideal. He initialization randomly selects weights from a Gaussian distribution with mean 0 and variance 2/n, where n is the number of inputs to the neuron. Using the same value for all weights wouldn't leverage the randomness needed for effective learning.\n",
        "\n",
        "2. **Is it OK to initialize the bias terms to 0?**\n",
        "   - Yes, initializing bias terms to 0 is generally fine. This is because biases affect the output of neurons linearly and don’t influence the gradients during backpropagation. Initializing biases to 0 simplifies the model without affecting its learning capacity.\n",
        "\n",
        "3. **Name three advantages of the SELU activation function over ReLU.**\n",
        "   - Advantages of SELU over ReLU:\n",
        "     - SELU can self-normalize, maintaining mean output close to 0 and variance close to 1, which helps in stabilizing training.\n",
        "     - It avoids dead neurons in deep networks by handling negative values more effectively.\n",
        "     - SELU is designed to preserve gradients, reducing the likelihood of exploding or vanishing gradients.\n",
        "\n",
        "4. **In which cases would you want to use each of the following activation functions: SELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?**\n",
        "   - **SELU:** For deep neural networks where self-normalization is beneficial.\n",
        "   - **Leaky ReLU:** When you want to mitigate the dying ReLU problem.\n",
        "   - **ReLU:** As a default choice for most hidden layers due to its simplicity and effectiveness.\n",
        "   - **Tanh:** When you need outputs ranging from -1 to 1, typically in hidden layers or output for regression tasks.\n",
        "   - **Logistic:** In the output layer for binary classification tasks, outputting probabilities.\n",
        "   - **Softmax:** In the output layer for multi-class classification tasks, outputting class probabilities that sum to 1.\n",
        "\n",
        "5. **What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer?**\n",
        "   - Setting momentum too close to 1 can cause the optimizer to overshoot the minimum point in the gradient descent process. This leads to unstable training, with the optimizer bouncing around the minimum rather than converging steadily.\n",
        "\n",
        "6. **Name three ways you can produce a sparse model.**\n",
        "   - Ways to produce a sparse model:\n",
        "     - **L1 regularization:** Penalizes large weights, encouraging the model to use only the most important features.\n",
        "     - **Dropout:** Randomly drops neurons during training, forcing the network to use different pathways and potentially creating sparsity.\n",
        "     - **Pruning:** Removing connections or neurons that contribute less to the model's performance, reducing redundancy and promoting sparsity.\n",
        "\n",
        "7. **Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)? What about MC Dropout?**\n",
        "   - **Dropout:** Yes, dropout slows down training because it requires more iterations to converge. It doesn’t affect inference speed since it’s only used during training.\n",
        "   - **MC Dropout:** During inference, MC Dropout requires multiple forward passes with dropout active to compute predictions with uncertainty estimates, which can be slower but provides valuable uncertainty information.\n",
        "\n",
        "8. **Practice training a deep neural network on the CIFAR10 image dataset:**\n",
        "   - Please let me know if you'd like to proceed with this detailed exercise."
      ],
      "metadata": {
        "id": "dHhfdItUNykm"
      }
    }
  ]
}