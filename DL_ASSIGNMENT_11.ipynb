{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQAqDu7nhw8pERNe6WjRbM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sanjay030303/Full-Stack-Data-Science-2023/blob/main/DL_ASSIGNMENT_11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEbbcABmu5Cw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Write the Python code to implement a single neuron.**\n",
        "   ```python\n",
        "   def single_neuron(inputs, weights, bias):\n",
        "       return sum(i * w for i, w in zip(inputs, weights)) + bias\n",
        "\n",
        "   # Example usage\n",
        "   inputs = [1.0, 2.0, 3.0]\n",
        "   weights = [0.1, 0.2, 0.3]\n",
        "   bias = 0.5\n",
        "   output = single_neuron(inputs, weights, bias)\n",
        "   print(output)  # Output: 1.4\n",
        "   ```\n",
        "\n",
        "2. **Write the Python code to implement ReLU.**\n",
        "   ```python\n",
        "   def relu(x):\n",
        "       return max(0, x)\n",
        "\n",
        "   # Example usage\n",
        "   print(relu(3.0))  # Output: 3.0\n",
        "   print(relu(-1.0)) # Output: 0\n",
        "   ```\n",
        "\n",
        "3. **Write the Python code for a dense layer in terms of matrix multiplication.**\n",
        "   ```python\n",
        "   import numpy as np\n",
        "\n",
        "   def dense_layer(inputs, weights, biases):\n",
        "       return np.dot(inputs, weights) + biases\n",
        "\n",
        "   # Example usage\n",
        "   inputs = np.array([1.0, 2.0, 3.0])\n",
        "   weights = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])\n",
        "   biases = np.array([0.5, 0.6])\n",
        "   output = dense_layer(inputs, weights, biases)\n",
        "   print(output)  # Output: [2.3 2.8]\n",
        "   ```\n",
        "\n",
        "4. **Write the Python code for a dense layer in plain Python (that is, with list comprehensions and functionality built into Python).**\n",
        "   ```python\n",
        "   def dense_layer_plain(inputs, weights, biases):\n",
        "       outputs = []\n",
        "       for i in range(len(weights[0])):  # number of outputs\n",
        "           output = sum(inputs[j] * weights[j][i] for j in range(len(inputs))) + biases[i]\n",
        "           outputs.append(output)\n",
        "       return outputs\n",
        "\n",
        "   # Example usage\n",
        "   inputs = [1.0, 2.0, 3.0]\n",
        "   weights = [[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]]\n",
        "   biases = [0.5, 0.6]\n",
        "   output = dense_layer_plain(inputs, weights, biases)\n",
        "   print(output)  # Output: [2.3, 2.8]\n",
        "   ```\n",
        "\n",
        "5. **What is the “hidden size” of a layer?**\n",
        "   - The \"hidden size\" of a layer refers to the number of neurons or units in a hidden layer of a neural network. It determines the dimensionality of the hidden layer's output.\n",
        "\n",
        "6. **What does the `t` method do in PyTorch?**\n",
        "   - The `t` method in PyTorch transposes a 2D tensor, swapping its rows and columns.\n",
        "   ```python\n",
        "   import torch\n",
        "   x = torch.tensor([[1, 2], [3, 4]])\n",
        "   print(x.t())  # Output: tensor([[1, 3], [2, 4]])\n",
        "   ```\n",
        "\n",
        "7. **Why is matrix multiplication written in plain Python very slow?**\n",
        "   - Matrix multiplication in plain Python is slow due to the lack of optimization for numerical operations, overhead of interpreted loops, and inefficiency in memory access patterns. Libraries like NumPy leverage optimized, compiled code and efficient memory handling.\n",
        "\n",
        "8. **In `matmul`, why is `ac==br`?**\n",
        "   - In matrix multiplication (A @ B = C), `ac` (number of columns of A) must equal `br` (number of rows of B) because the dot product is computed between rows of A and columns of B, which requires them to be the same length.\n",
        "\n",
        "9. **In Jupyter Notebook, how do you measure the time taken for a single cell to execute?**\n",
        "   - You can use the `%time` or `%timeit` magic command to measure execution time.\n",
        "   ```python\n",
        "   %time result = sum(range(1000))\n",
        "   ```\n",
        "\n",
        "10. **What is elementwise arithmetic?**\n",
        "    - Elementwise arithmetic refers to performing arithmetic operations on corresponding elements of arrays or tensors. For example, adding two vectors elementwise means adding each corresponding pair of elements.\n",
        "\n",
        "11. **Write the PyTorch code to test whether every element of `a` is greater than the corresponding element of `b`.**\n",
        "    ```python\n",
        "    import torch\n",
        "\n",
        "    a = torch.tensor([1, 2, 3])\n",
        "    b = torch.tensor([0, 2, 1])\n",
        "    result = a > b\n",
        "    print(result)  # Output: tensor([True, False, True])\n",
        "    ```\n",
        "\n",
        "12. **What is a rank-0 tensor? How do you convert it to a plain Python data type?**\n",
        "    - A rank-0 tensor, also known as a scalar tensor, has no dimensions. You can convert it to a plain Python data type using the `item` method.\n",
        "    ```python\n",
        "    import torch\n",
        "    scalar = torch.tensor(5)\n",
        "    python_scalar = scalar.item()\n",
        "    print(python_scalar)  # Output: 5\n",
        "    ```\n",
        "\n",
        "13. **How does elementwise arithmetic help us speed up `matmul`?**\n",
        "    - Elementwise arithmetic allows for parallel computation on hardware like GPUs, improving efficiency. In `matmul`, leveraging elementwise operations can optimize intermediate steps, reducing computation time.\n",
        "\n",
        "14. **What are the broadcasting rules?**\n",
        "    - Broadcasting rules allow arrays of different shapes to be used in arithmetic operations by automatically expanding their shapes to be compatible. The rules are:\n",
        "      1. If the arrays differ in rank, prepend the shape of the smaller-rank array with ones.\n",
        "      2. Compare the shapes elementwise, starting from the trailing dimensions.\n",
        "      3. Dimensions must either be equal, one of them must be 1, or the size of the dimension must be compatible.\n",
        "\n",
        "15. **What is `expand_as`? Show an example of how it can be used to match the results of broadcasting.**\n",
        "    - `expand_as` in PyTorch expands a tensor to the same size as another tensor.\n",
        "    ```python\n",
        "    import torch\n",
        "\n",
        "    x = torch.tensor([1, 2, 3])\n",
        "    y = torch.tensor([[4, 5, 6], [7, 8, 9]])\n",
        "    x_expanded = x.expand_as(y)\n",
        "    print(x_expanded)\n",
        "    # Output:\n",
        "    # tensor([[1, 2, 3],\n",
        "    #         [1, 2, 3]])\n",
        "    ```\n",
        "\n",
        "    This matches the results of broadcasting by explicitly expanding `x` to the shape of `y`."
      ],
      "metadata": {
        "id": "nmUV5WIcu50E"
      }
    }
  ]
}